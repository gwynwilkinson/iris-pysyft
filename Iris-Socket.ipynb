{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import syft\n",
    "from syft import WebsocketClientWorker\n",
    "from syft import WebsocketServerWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = syft.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.batch_size = 8\n",
    "        self.test_batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "\n",
    "args = Parser()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset):\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "\n",
    "    X_train = torch.from_numpy(X).float()\n",
    "    y_train = torch.from_numpy(y).float()\n",
    "    X_test = torch.from_numpy(X).float()\n",
    "    y_test = torch.from_numpy(y).float()\n",
    "    \n",
    "    train = TensorDataset(X_train, y_train)\n",
    "    test = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = DataLoader(test, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = prepare_data(iris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # define nn\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:syft.workers.base:Worker alice already exists. Replacing old worker which could cause                     unexpected behavior\n",
      "WARNING:syft.workers.base:Worker bob already exists. Replacing old worker which could cause                     unexpected behavior\n"
     ]
    }
   ],
   "source": [
    "#bob = syft.VirtualWorker(hook, id=\"bob\")\n",
    "#alice = syft.VirtualWorker(hook, id=\"alice\")\n",
    "\n",
    "alice = WebsocketClientWorker(host=\"localhost\",hook=hook,id=\"alice\",port=8182)\n",
    "bob   = WebsocketClientWorker(host=\"localhost\",hook=hook,id=\"bob\",port=8183)\n",
    "\n",
    "secure_worker = WebsocketServerWorker(host=\"localhost\",hook=hook,id=\"secure_worker\",port=8181)\n",
    "\n",
    "hook = syft.TorchHook(torch, local_worker=secure_worker)\n",
    "hook.local_worker.add_worker(alice)\n",
    "hook.local_worker.add_worker(bob)\n",
    "\n",
    "\n",
    "compute_nodes = [bob, alice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_remote_dataset():\n",
    "\n",
    "    # remote dataset contains 2 lists, of pointers to Alices and Bobs datasets respectively\n",
    "    # TODO redo this to work with arbitrary numbers of workers\n",
    "    remote_dataset = (list(), list())\n",
    "\n",
    "    for batch_index, (data,target) in enumerate(train_loader):\n",
    "\n",
    "        # Send data,target to the worker_id by doing modulo num_workers\n",
    "        data = data.send(compute_nodes[batch_index % len(compute_nodes)])      \n",
    "        target = target.send(compute_nodes[batch_index % len(compute_nodes)])\n",
    "\n",
    "        # remote_dataset is  list of pointers to the locations of each worker's list of data,target\n",
    "        remote_dataset[batch_index % len(compute_nodes)].append((data,target))\n",
    "        \n",
    "    return remote_dataset\n",
    "remote_dataset = init_remote_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs one remote training update on the model using remote data\n",
    "def update(data, target, model, optimizer):\n",
    "    \n",
    "    # send the model to the data owner\n",
    "    model = model.send(data.location)\n",
    "    \n",
    "    # Perform training update\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.mse_loss(pred.view(-1), target)\n",
    "    #print(\"\\n\",loss)\n",
    "    loss = loss.get()\n",
    "    #print(loss, \"\\n\")\n",
    "    # TODO: Figure out why step() doesn't appear to be updating the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Retrieve model from data owner\n",
    "    model = model.get()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hack(data, target, model, optimizer):\n",
    "    \n",
    "    owner = data.location\n",
    "    \n",
    "    # Retrieve data from data owners\n",
    "    data = data.get()\n",
    "    target = target.get()\n",
    "    \n",
    "    # Perform training update\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.mse_loss(pred.view(-1), target)\n",
    "    \n",
    "    # TODO: Figure out why this isn't updating the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    data = data.send(owner)\n",
    "    target = target.send(owner)\n",
    "       \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init models & optimizers for each remote worker, and combine in a list\n",
    "# TODO: rewrite this to work with arbitrary number of workers\n",
    "alices_model = Net()\n",
    "bobs_model = Net()\n",
    "\n",
    "alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr)\n",
    "bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr)\n",
    "\n",
    "models = [alices_model, bobs_model]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]\n",
    "\n",
    "params = [list(bobs_model.parameters()), list(alices_model.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    remote_dataset = init_remote_dataset()\n",
    "    \n",
    "    for data_index in range(len(remote_dataset[0])-1):\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            \n",
    "            # Retrieve data,target pointers from remote dataset\n",
    "            data, target = remote_dataset[remote_index][data_index]\n",
    "            \n",
    "            # Update the respective model\n",
    "            #TODO: Figure out why this doesn't work (see update function def)\n",
    "            models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "            #models[remote_index] = update_hack(data, target, models[remote_index], optimizers[remote_index])\n",
    "\n",
    "    # New list to hold aggregated parameters for each layer of our new model\n",
    "    # TODO: rewrite this part to work with arbitrary number of workers and remove hardcoded division by 2\n",
    "    param_agg = list()\n",
    "        \n",
    "    for layer_index in range(len(params[0])):\n",
    "            \n",
    "            # Copy both workers' parameter layers, convert to fixed-prec and encrypt using additive secret-sharing, then add together\n",
    "            #param_agg.append(params[0][layer_index].data.copy().fix_prec().share(bob, alice, crypto_provider=secure_worker) + params[1][layer_index].data.copy().fix_prec().share(bob, alice, crypto_provider=secure_worker))\n",
    "            param_agg.append(params[0][layer_index].data.copy() + params[1][layer_index].data.copy())\n",
    "            \n",
    "            # Decrypt the summed parameter layer, convert back to float-prec, and divide by 2 to get the average of both models\n",
    "            #param_agg[layer_index] = param_agg[layer_index].get().float_precision()/2\n",
    "            param_agg[layer_index] = param_agg[layer_index]/2\n",
    "            \n",
    "    \n",
    "    # Disable autograd and replace worker parameters with new aggregated parameters\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Zero all model parameters\n",
    "        for model in params:\n",
    "            for param in model:\n",
    "                param *= 0\n",
    "        \n",
    "        # Set each parameter to the value of its aggregated counterpart\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "                for layer_index in range(len(params[remote_index])):\n",
    "                    params[remote_index][layer_index].set_(param_agg[layer_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function copied from tutorial\n",
    "# TODO: Inspect and comment this function\n",
    "def test():\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        output = models[0](data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Test set: Average loss: 2.3970\n",
      "\n",
      "Total 23.51 s\n"
     ]
    }
   ],
   "source": [
    "# Init new timer to measure training time\n",
    "t = time.time()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train()\n",
    "    test()\n",
    "    \n",
    "# Output training time\n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.3970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, target = iris.data, iris.target\n",
    "\n",
    "# data = torch.from_numpy(data).float()\n",
    "# target = torch.from_numpy(target).float()\n",
    "\n",
    "# alices_optimizer.zero_grad()\n",
    "\n",
    "# pred = alices_model(data)\n",
    "# loss = F.mse_loss(pred.view(-1), target)\n",
    "# loss.backward()\n",
    "# alices_optimizer.step()\n",
    "# print(loss)\n",
    "# alice_params = list(alices_model.parameters())\n",
    "# alice_params[0].data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
